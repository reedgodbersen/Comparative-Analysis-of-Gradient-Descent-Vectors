# Comparative-Analysis-of-Gradient-Descent-Vectors

Code is taken from my EECS 453 class project in which Hyune June Kim and I write the paper: "Comparative Analysis of Gradient Descent Variants".

Abstract of the paper:

This paper conducts a detailed examination of the Gradient Descent (GD) algorithm, a
cornerstone of optimization in machine learning. We focus on the mathematical foundation
and convergence properties of GD, particularly analyzing the influence of the learning rate—a
critical hyperparameter—on the algorithm’s efficiency and stability. The paper highlights the
adaptations of GD, including Stochastic Gradient Descent (SGD) and Mini-Batch Gradient
Descent, addressing the computational challenges in large-scale data settings. Additionally, it
explores advanced variants like momentum-based GD and the Adam optimizer, which enhance
convergence in complex optimization landscapes. We aim to provide insights into the practical
applications and limitations of GD in various machine learning contexts, offering a comprehen-
sive understanding of this fundamental optimization tool in the broader framework of artificial
intelligence and data science.
